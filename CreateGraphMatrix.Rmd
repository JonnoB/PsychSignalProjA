---
title: "Retweets Absolute Bulllisness Adjacency"
author: "Jonathan Bourne"
date: "23 February 2016"
output: html_document
---


```{r}
packages <- c("dplyr", "tidyr", "ggplot2", "caret", "corrplot", "xtable", "magrittr", "zoo", "igraph", "rgefx", "MCL", "Hmisc", "gridExtra")
sapply(packages, library, character.only = TRUE)


basewd <- "C:/Users/Jonno/Dropbox/Data_Analytics" #change this to your dropbox file path for data analytics
DataFiles <- file.path(basewd, "Data")
GraphicsFiles <- file.path(basewd, "Graphics", "Milestone2")

```


Import data
```{r}
setwd(DataFiles)
TwitRetweets <- read.csv(gzfile("twitter_withretweets_daily.csv.gz"), as.is = TRUE)
symbology <- read.csv("symbology.csv")
setwd(GraphicsFiles)
```

#Data Preparation

Reshape the twitter data into wide form, remove weekends and holidays?
```{r}
TwitBull <- TwitRetweets %>% select(SYMBOL, TIMESTAMP_UTC, BULLISH_INTENSITY) %>% 
  spread(key = SYMBOL, value = BULLISH_INTENSITY, fill = 0) #change metric
setwd(DataFiles)

```

make a histogram of zero entries after no preprocessing, removing near zero, removing less than 70% full
```{r}

NoPreProcTwit <- TwitBull %>% gather(key = symbol, value = tweets, -TIMESTAMP_UTC)
NearZeroTwit <- TwitBull[,-nearZeroVar(TwitBull)] %>% gather(key = symbol, value = tweets, -TIMESTAMP_UTC)
#remove symbols with less than 5/7% 
viable <- colSums(TwitBull>0) / nrow(TwitBull) > 5/7
HighFillTwit <- TwitBull[,viable] %>% gather(key = symbol, value = tweets, -TIMESTAMP_UTC)


NoPreProcplot <- ggplot(NoPreProcTwit, aes(tweets, ..density..)) + 
  geom_histogram(bins = 20)+ggtitle("No PreProcessing")

NearZeroplot <- ggplot(NearZeroTwit, aes(tweets, ..density..)) + 
  geom_histogram(bins = 20) +ggtitle("Near Zero Removed")

HighFillplot <- ggplot(HighFillTwit, aes(tweets,..density..)) + 
  geom_histogram(bins = 20) +ggtitle("Remove Less than 70% Fill")

setwd(GraphicsFiles)
gridfig <- grid.arrange(NoPreProcplot, NearZeroplot, HighFillplot, nrow= 1)
ggsave("ZerosDensity.png", gridfig, width = 8, height = 5.25)

rm("NoPreProcTwit","NearZeroTwit", "HighFillTwit", "NoPreProcplot","NearZeroplot","HighFillplot", "gridfig")

```


Remove variables which have less than 70% of rows with values in. Save as csv
```{r}

TwitBullvar <- TwitBull[,viable]
TwitBullvar %<>% mutate(TIMESTAMP_UTC = as.Date(TIMESTAMP_UTC))
#write.csv(TwitBull, file = "TwitBullWide.csv") #save under new type with correct metric

```


Seperate into time periods and create a single test time period
```{r}
periodID <- rep(1:ceiling(nrow(TwitBullvar)/60), length.out = nrow(TwitBullvar), each = 60)
testperiod <- 27
data <- TwitBullvar[periodID == testperiod,]

#DataZeroVar <- nearZeroVar(data)
#data <- data[,-DataZeroVar]
```

Decompose to enforce stationarity in each timeseries, this will create a large list of decomposed data. Also create a matrix/matrices of the decomposed data.
```{r}
#  <- lapply(data, function(n) {
#   
# })

x <- ts(data[,2], frequency = 7 )
x <- stats::decompose(x)
```

We didn't remove the trend because over a two month period there is not enough time for changes in twitter volume and so removing any apparent trend could be more related to major events than any general trend

Create distance and significance matrices
```{r}
corlist <- data[,2:ncol(data)] %>% as.matrix %>% rcorr
distmat <- sqrt(2*(1-corlist[[1]]))

#number of links for given significance value
cutoff <- data.frame(cutoff = seq(0.75,1,0.01), edges = NA)

cutoff$edges <- sapply(cutoff$cutoff ,function(n) {
  sum(corlist[[3]]>n, na.rm = TRUE)/2
  })

ggplot(cutoff, aes(x= cutoff, y= edges)) + geom_line() +ggtitle("Edge Number is inversely proportional to cut off point") +xlab("Cut Off") +ylab(" Number of Edges")
ggsave("Edgenumber.png")

```

Merge Distance and Adjacency matrices to create a weighted undirected graph
```{r}

sigmat <- corlist[[3]]>0.75
MNet <- distmat*sigmat #weighted undirected adjacency matrix
write.csv(MNet, "MNet.csv")

histogram(density(MNet, na.rm = TRUE))

x <- MNet %>% data.frame %>% gather(key = SYMBOL, value = distance) %>% 
  filter( distance = !is.na(distance))

setwd(GraphicsFiles)
ggplot(x,aes(x= distance, ..density..)) +geom_histogram(binwidth = 0.05) +ggtitle("Distribution of distances")
ggsave("Distancedistrib.png")

#the distribution  of number of node connection  
Kdist <- data.frame(K = rowSums(sigmat, na.rm = TRUE) )
ggplot(Kdist, aes(K, ..density..)) + 
  geom_histogram(bins = 20) +ggtitle("Distriution of Node Degree")+xlab("Number of edges")
ggsave("Kdistrib.png")

```

Run Various Clustering Algorithms
```{r}

MNet2 <- MNet #MCL doesn't use igraph structures
MNet <- graph.adjacency(MNet, mode = "undirected", weighted = TRUE, diag = FALSE)

com1 <- fastgreedy.community(MNet)
com2 <- cluster_walktrap(MNet)
com3 <- cluster_spinglass(MNet)
com4 <- cluster_edge_betweenness(MNet)
com5 <- mcl(x = MNet2, addLoops=TRUE, ESM = TRUE)

```


Create a table for the time period with the metric for each day and the cluster ID
```{r}
symclust <- data.frame(SYMBOL = names(data)[-1], ClusterID = com5$Cluster)
Twitaggmat <- TwitRetweets %>% select(SYMBOL, TIMESTAMP_UTC, BULLISH_INTENSITY) %>% 
  inner_join(., symclust, "SYMBOL")

write.csv(Twitaggmat, "Twitaggmat.csv")
```


Function to create list of each period
```{r}

NumPeriods <- 1:ceiling(nrow(TwitBullvar)/60)
periodID <- rep(1:ceiling(nrow(TwitBullvar)/60), length.out = nrow(TwitBullvar), each = 60)


TwitMatList <- lapply(NumPeriods, function(n) {

  data = TwitBullvar[periodID == n,]
  remove = nearZeroVar(data)
  if(length(remove) >0){
  data = data[,-remove]
  }
  corlist = data[,2:ncol(data)] %>% as.matrix %>% rcorr
  distmat = sqrt(2*(1-corlist[[1]]))
  sigmat = corlist[[3]]>0.75
  MNet = distmat*sigmat #weighted undirected adjacency matrix
  #MNet = graph.adjacency(MNet, mode = "undirected", weighted = TRUE, diag = FALSE)#do not use if clustering with mcl
  com5 = mcl(x = MNet, addLoops=TRUE, ESM = FALSE)
  symclust = data.frame(SYMBOL = names(data)[-1], ClusterID = com5$Cluster)
  Twitaggmat = TwitRetweets %>% select(SYMBOL, TIMESTAMP_UTC, BULLISH_INTENSITY) %>% 
    inner_join(., symclust, "SYMBOL")
  print(n)
  Twitaggmat
  
  }
)

# saveRDS(TwitMatList, "TwitMatList.rds")
# TwitMatList <-readRDS("TwitMatList.rds")
# setwd(file.path(DataFiles, "TweetMatPeriods"))
# lapply(3:32, function(n) {
#       write.csv(TwitMatList[[n]], paste("Twitmat_Period",n, ".csv", sep="") )
#   print(n)
# })
TwitMatList <- TwitMatAgg
```

Aggregate list into unweighted time periods and combine into single dataframe
```{r}
TwitMatAgg<- lapply(1:length(TwitMatList), function(m) {
  
x <- TwitMatList[[m]] %>% group_by(ClusterID) %>% summarise(mean = mean(BULLISH_INTENSITY)) %>% ungroup
 x2 <- TwitMatList[[m]] %>% group_by(SYMBOL) %>% 
   summarise(ClusterID = first(ClusterID)) %>% left_join(., x, by = "ClusterID") %>%
   mutate(PeriodID = m)
}
)
TwitMatAgg <- rbind_all(TwitMatAgg)

m <- 14
x <- TwitMatList[[m]] %>% group_by(ClusterID) %>% summarise(mean = mean(BULLISH_INTENSITY)) %>% ungroup
 x2 <- TwitMatList[[m]] %>% group_by(SYMBOL) %>% 
   summarise(ClusterID = first(ClusterID)) %>% left_join(., x, by = "ClusterID") %>%
   mutate(PeriodID = m)

```

Plot unweighted Social Index across periods
```{r}

target <-  TwitMatAgg$SYMBOL %in% c("YHOO", "AAPL", "AIG", "COMPQ","EBAY","GOLD", "GOOG", "MSFT", "USD")
 
ggplot(TwitMatAgg[target,], aes(x= PeriodID, y = mean, colour = SYMBOL)) + geom_line() +
  ggtitle("Comparing Unweighted Social INdex behaviour across time") + ylab("Mean BUllish Intensity") +xlab("Period Number")

ggplot(TwitMatAgg, aes(x= PeriodID, y = mean, colour = SYMBOL)) + geom_line() +
  ggtitle("Comparing Unweighted Social INdex behaviour across time") + ylab("Mean BUllish Intensity") +xlab("Period Number") +theme(legend.position = "none")
```


Structure graph for use in gephi with as much metadata as possible
symbol, cluster ID, edge type (aka inter cluster vs intra cluster), is target symbol/node yesy/no etc
```{r}

```

