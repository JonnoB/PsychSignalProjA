---
title: "Retweets Absolute Bulllisness Adjacency"
author: "Jonathan Bourne"
date: "23 February 2016"
output: html_document
---


```{r}
packages <- c("dplyr", "tidyr", "ggplot2", "caret", "corrplot", "xtable", "magrittr", "zoo", "igraph", "rgefx", "MCL", "Hmisc")
sapply(packages, library, character.only = TRUE)


basewd <- "C:/Users/Jonno/Dropbox/Data_Analytics" #change this to your dropbox file path for data analytics
DataFiles <- file.path(basewd, "Data")
GraphicsFiles <- file.path(basewd, "Graphics", "Milestone2")

```


Import data
```{r}
setwd(DataFiles)
TwitRetweets <- read.csv(gzfile("twitter_withretweets_daily.csv.gz"), as.is = TRUE)
symbology <- read.csv("symbology.csv")
setwd(GraphicsFiles)
```

#Data Preparation

Reshape the twitter data into wide form, remove weekends and holidays?
```{r}
TwitBull <- TwitRetweets %>% select(SYMBOL, TIMESTAMP_UTC, BULLISH_INTENSITY) %>% 
  spread(key = SYMBOL, value = BULLISH_INTENSITY, fill = 0) #change metric
setwd(DataFiles)

```


Remove variables of near zero variance. Save as csv
```{r}

#remove symbols with less than 5/7% 
viable <- colSums(TwitBull>0) / nrow(TwitBull) > 5/7
sum(viable)

TwitBullvar <- TwitBull[,viable]

TwitBullvar %<>% mutate(TIMESTAMP_UTC = as.Date(TIMESTAMP_UTC))

#Histogram of the distriubution of tweet intensity for the selected symbols
x <- TwitBullvar %>% gather(key = symbol, value = tweets, -TIMESTAMP_UTC)
histogram(x$tweets)
#write.csv(TwitBull, file = "TwitBullWide.csv") #save under new type with correct metric

```


Seperate into time periods and create a single test time period
```{r}
periodID <- rep(1:ceiling(nrow(TwitBullvar)/60), length.out = nrow(TwitBullvar), each = 60)

testperiod <- 27

data <- TwitBullvar[periodID == testperiod,]

#DataZeroVar <- nearZeroVar(data)
#data <- data[,-DataZeroVar]
```

Decompose to enforce stationarity in each timeseries, this will create a large list of decomposed data. Also create a matrix/matrices of the decomposed data.
```{r}
#  <- lapply(data, function(n) {
#   
# })

x <- ts(data[,2], frequency = 7 )
x <- stats::decompose(x)
```

We didn't remove the trend because over a two month period there is not enough time for changes in twitter volume and so removing any apparent trend could be more related to major events than any general trend

Create distance and significance matrices
```{r}
corlist <- data[,2:ncol(data)] %>% as.matrix %>% rcorr
distmat <- sqrt(2*(1-corlist[[1]]))

#number of links for given significance value
cutoff <- data.frame(cutoff = seq(0.75,1,0.01), edges = NA)

cutoff$edges <- sapply(cutoff$cutoff ,function(n) {
  sum(corlist[[3]]>n, na.rm = TRUE)/2
  })

ggplot(cutoff, aes(x= cutoff, y= edges)) + geom_line() +ggtitle("Edges decreast Linearly with increaing cutoff point")


```

Merge Distance and Adjacency matrices to create a weighted undirected graph
```{r}

sigmat <- corlist[[3]]>0.75
MNet <- distmat*sigmat #weighted undirected adjacency matrix
write.csv(MNet, "MNet.csv")

plot(density(rowSums(sigmat, na.rm = TRUE))) #thedistribution  of number of node connection  

plot(density(MNet, na.rm = TRUE))

MNet <- graph.adjacency(MNet, mode = "undirected", weighted = TRUE, diag = FALSE)

com1 <- fastgreedy.community(MNet)
com2 <- cluster_walktrap(MNet)
com3 <- cluster_spinglass(MNet)
com4 <- cluster_edge_betweenness(MNet)
com5 <- mcl(x = MNet, addLoops=TRUE, ESM = TRUE)

```

Create a table for the time period with the metric for each day and the cluster ID
```{r}
symclust <- data.frame(SYMBOL = names(data)[-1], ClusterID = com5$Cluster)
Twitaggmat <- TwitRetweets %>% select(SYMBOL, TIMESTAMP_UTC, BULLISH_INTENSITY) %>% 
  inner_join(., symclust, "SYMBOL")

write.csv(Twitaggmat, "Twitaggmat.csv")
```


Function to create list of each period
```{r}

NumPeriods <- 1:ceiling(nrow(TwitBullvar)/60)
periodID <- rep(1:ceiling(nrow(TwitBullvar)/60), length.out = nrow(TwitBullvar), each = 60)


TwitMatList <- lapply(NumPeriods, function(n) {

  data = TwitBullvar[periodID == n,]
  remove = nearZeroVar(data)
  if(length(remove) >0){
  data = data[,-remove]
  }
  corlist = data[,2:ncol(data)] %>% as.matrix %>% rcorr
  distmat = sqrt(2*(1-corlist[[1]]))
  sigmat = corlist[[3]]>0.75
  MNet = distmat*sigmat #weighted undirected adjacency matrix
  #MNet = graph.adjacency(MNet, mode = "undirected", weighted = TRUE, diag = FALSE)#do not use if clustering with mcl
  com5 = mcl(x = MNet, addLoops=TRUE, ESM = TRUE)
  symclust = data.frame(SYMBOL = names(data)[-1], ClusterID = com5$Cluster)
  Twitaggmat = TwitRetweets %>% select(SYMBOL, TIMESTAMP_UTC, BULLISH_INTENSITY) %>% 
    inner_join(., symclust, "SYMBOL")
  print(n)
  Twitaggmat
  
  }
)

# saveRDS(TwitMatList, "TwitMatList.rds")
# 
# setwd(file.path(DataFiles, "TweetMatPeriods"))
# lapply(3:32, function(n) {
#       write.csv(TwitMatList[[n]], paste("Twitmat_Period",n, ".csv", sep="") )
#   print(n)
# })

```

Aggregate list into unweighted time periods and combine into single dataframe
```{r}
TwitMatAgg<- lapply(1:length(TwitMatList), function(m) {
  
x <- TwitMatList[[m]] %>% group_by(ClusterID) %>% summarise(mean = mean(BULLISH_INTENSITY)) %>% ungroup
 x2 <- TwitMatList[[m]] %>% group_by(SYMBOL) %>% 
   summarise(ClusterID = first(ClusterID)) %>% left_join(., x, by = "ClusterID") %>%
   mutate(PeriodID = m)
}
)
TwitMatAgg <- rbind_all(TwitMatAgg)



```

Plot unweighted Social Index across periods
```{r}

target <-  TwitMatAgg$SYMBOL %in% c("YHOO", "AAPL", "AIG", "COMPQ","EBAY","GOLD", "GOOG", "MSFT", "USD")
 
ggplot(TwitMatAgg[target,], aes(x= PeriodID, y = mean, colour = SYMBOL)) + geom_line() +
  ggtitle("Comparing Unweighted Social INdex behaviour across time") + ylab("Mean BUllish Intensity") +xlab("Period Number")
```


Structure graph for use in gephi with as much metadata as possible
symbol, cluster ID, edge type (aka inter cluster vs intra cluster), is target symbol/node yesy/no etc
```{r}

```

