---
title: "Retweets Absolute Bulllisness Adjacency"
author: "Jonathan Bourne"
date: "23 February 2016"
output: html_document
---


```{r}
packages <- c("dplyr", "tidyr", "ggplot2", "caret", "corrplot", "xtable", "magrittr", "zoo", "igraph", "rgexf", "MCL", "Hmisc", "gridExtra", "Quandl", "xtable")
sapply(packages, library, character.only = TRUE)


basewd <- "C:/Users/Jonno/Dropbox/Data_Analytics" #change this to your dropbox file path for data analytics
DataFiles <- file.path(basewd, "Data")
GraphicsFiles <- file.path(basewd, "Graphics", "Milestone2")

```


Import data
```{r}
setwd(DataFiles)
TwitRetweets <- read.csv(gzfile("twitter_withretweets_daily.csv.gz"), as.is = TRUE)
symbology <- read.csv("symbology.csv")
setwd(GraphicsFiles)
```

#Data Preparation

Reshape the twitter data into wide form, remove weekends and holidays?
```{r}

Metric <- "BEARISH_INTENSITY"

TwitBull <- TwitRetweets %>% select_("SYMBOL", "TIMESTAMP_UTC", Metric) %>% 
  rename_("Metric" = Metric) %>% 
  spread(key = SYMBOL, value = Metric, fill = 0) #change metric
setwd(DataFiles)

```

make a histogram of zero entries after no preprocessing, removing near zero, removing less than 70% full
```{r}

NoPreProcTwit <- TwitBull %>% gather(key = symbol, value = tweets, -TIMESTAMP_UTC)
NearZeroTwit <- TwitBull[,-nearZeroVar(TwitBull)] %>% gather(key = symbol, value = tweets, -TIMESTAMP_UTC)
#remove symbols with less than 5/7% 
viable <- colSums(TwitBull>0) / nrow(TwitBull) > 5/7
HighFillTwit <- TwitBull[,viable] %>% gather(key = symbol, value = tweets, -TIMESTAMP_UTC)


NoPreProcplot <- ggplot(NoPreProcTwit, aes(tweets, ..density..)) + 
  geom_histogram(bins = 20)+ggtitle("No PreProcessing")

NearZeroplot <- ggplot(NearZeroTwit, aes(tweets, ..density..)) + 
  geom_histogram(bins = 20) +ggtitle("Near Zero Removed")

HighFillplot <- ggplot(HighFillTwit, aes(tweets,..density..)) + 
  geom_histogram(bins = 20) +ggtitle("At least 70% Fill")

setwd(GraphicsFiles)
gridfig <- grid.arrange(NoPreProcplot, NearZeroplot, HighFillplot, nrow= 1)
ggsave("ZerosDensity.png", gridfig, width = 8, height = 5.25)

rm("NoPreProcTwit","NearZeroTwit", "HighFillTwit", "NoPreProcplot","NearZeroplot","HighFillplot", "gridfig")

```


Remove variables which have less than 70% of rows with values in. Save as csv
```{r}

TwitBullvar <- TwitBull[,viable]
TwitBullvar %<>% mutate(TIMESTAMP_UTC = as.Date(TIMESTAMP_UTC))
#write.csv(TwitBull, file = "TwitBullWide.csv") #save under new type with correct metric

```


Seperate into time periods and create a single test time period
```{r}
periodID <- rep(1:ceiling(nrow(TwitBullvar)/60), length.out = nrow(TwitBullvar), each = 60)
testperiod <- 27
data <- TwitBullvar[periodID == testperiod,]

#DataZeroVar <- nearZeroVar(data)
#data <- data[,-DataZeroVar]
```

Decompose to enforce stationarity in each timeseries, this will create a large list of decomposed data. Also create a matrix/matrices of the decomposed data.
```{r}
#  <- lapply(data, function(n) {
#   
# })

x <- ts(data[,2], frequency = 7 )
x <- stats::decompose(x)
```

We didn't remove the trend because over a two month period there is not enough time for changes in twitter volume and so removing any apparent trend could be more related to major events than any general trend

Create distance and significance matrices
```{r}
corlist <- data[,2:ncol(data)] %>% as.matrix %>% rcorr
distmat <- sqrt(2*(1-corlist[[1]]))

#number of links for given significance value
cutoff <- data.frame(cutoff = seq(0.75,1,0.01), edges = NA)

cutoff$edges <- sapply(cutoff$cutoff ,function(n) {
  sum(corlist[[3]]>n, na.rm = TRUE)/2
  })

ggplot(cutoff, aes(x= cutoff, y= edges)) + geom_line() +ggtitle("Edge Number is inversely proportional to cut off point") +xlab("Cut Off") +ylab(" Number of Edges")
ggsave("Edgenumber.png")

```

Merge Distance and Adjacency matrices to create a weighted undirected graph
```{r}

sigmat <- corlist[[3]]>0.75
MNet <- distmat*sigmat #weighted undirected adjacency matrix
write.csv(MNet, "MNet.csv")

histogram(density(MNet, na.rm = TRUE))

x <- MNet %>% data.frame %>% gather(key = SYMBOL, value = distance) %>% 
  filter( distance = !is.na(distance))

setwd(GraphicsFiles)
ggplot(x,aes(x= distance, ..density..)) +geom_histogram(binwidth = 0.05) +ggtitle("Distribution of distances")
ggsave("Distancedistrib.png")

#the distribution  of number of node connection  
Kdist <- data.frame(K = rowSums(sigmat, na.rm = TRUE) )
ggplot(Kdist, aes(K, ..density..)) + 
  geom_histogram(bins = 20) +ggtitle("Distriution of Node Degree")+xlab("Number of edges")
ggsave("Kdistrib.png")

```

Run Various Clustering Algorithms
```{r}

MNet2 <- MNet #MCL doesn't use igraph structures
MNet <- graph.adjacency(MNet, mode = "undirected", weighted = TRUE, diag = FALSE)

com1 <- fastgreedy.community(MNet)
com2 <- cluster_walktrap(MNet)
com3 <- cluster_spinglass(MNet)
com4 <- cluster_edge_betweenness(MNet)
com5 <- mcl(x = MNet2, addLoops=TRUE, ESM = TRUE)

```


Create a table for the time period with the metric for each day and the cluster ID
```{r}
symclust <- data.frame(SYMBOL = names(data)[-1], ClusterID = com5$Cluster)
Twitaggmat <- TwitRetweets %>% select(SYMBOL, TIMESTAMP_UTC, Metric) %>% 
  inner_join(., symclust, "SYMBOL")

write.csv(Twitaggmat, "Twitaggmat.csv")
```

The function required to create the twitter matrix list
```{r}
TwitMatList <-function(TweetDataRaw, Metric = "BULLISH_INTENSITY"){
  TwitBull = TweetDataRaw %>% select_("SYMBOL", "TIMESTAMP_UTC", Metric) %>% 
    rename_("Metric" = Metric) %>% 
    spread(key = SYMBOL, value = Metric, fill = 0) %>% 
    mutate(TIMESTAMP_UTC = as.Date(TIMESTAMP_UTC))
  
  print("Metric select data set reshaped")
  
  viable = colSums(TwitBull>0) / nrow(TwitBull) > 5/7
  TwitBullvar = TwitBull[,viable]
  TwitBullvar %<>%  filter(TIMESTAMP_UTC > as.Date("2010-12-31"),TIMESTAMP_UTC < as.Date("2016-01-01") )
  
  print("Non-Viable symbols removed")
  
  NumPeriods <- 1:ceiling(nrow(TwitBullvar)/60)
  periodID <- rep(1:ceiling(nrow(TwitBullvar)/60), length.out = nrow(TwitBullvar), each = 60)


  TwitMatList <- lapply(NumPeriods, function(n) {
  
    data = TwitBullvar[periodID == n,]
    remove = nearZeroVar(data)
    if(length(remove) >0){
    data = data[,-remove]
    }
    corlist = data[,2:ncol(data)] %>% as.matrix %>% rcorr
    distmat = sqrt(2*(1-corlist[[1]]))
    sigmat = corlist[[3]]>0.75
    MNet = distmat*sigmat #weighted undirected adjacency matrix
    #MNet = graph.adjacency(MNet, mode = "undirected", weighted = TRUE, diag = FALSE)#do not use if clustering with mcl
    com5 = mcl(x = MNet, addLoops=TRUE, max.iter = 100,inflation = 2 , ESM = FALSE)
    symclust = data.frame(SYMBOL = names(data)[-1], ClusterID = com5$Cluster)
    Twitaggmat = TweetDataRaw %>% select_("SYMBOL", "TIMESTAMP_UTC", Metric) %>% 
      inner_join(., symclust, "SYMBOL")
    print(paste("Period",n, "complete"))
    Twitaggmat
    
    }
  )

}

```


Function: Aggregate list into unweighted time periods and combine into single dataframe
```{r}
Aggregator <- function(TwitMatList, Metric){
  TwitMatAgg<- lapply(1:length(TwitMatList), function(m) {
    #SUmmary uses non-standard evaluation so that the Metric can be properly evaluated using it's name
  x <- TwitMatList[[m]] %>% group_by(ClusterID) %>% summarise_(mean = (paste("mean(", Metric,")"))) %>% ungroup
   x2 <- TwitMatList[[m]] %>% group_by(SYMBOL) %>% 
     summarise(ClusterID = first(ClusterID)) %>% left_join(., x, by = "ClusterID") %>%
     mutate(PeriodID = m)
  }
  )
  TwitMatAgg <- rbind_all(TwitMatAgg)
}


#Testing purposes
# m <- 14
# x <- TwitMatList[[m]] %>% group_by(ClusterID) %>% summarise(mean = mean(Metric)) %>% ungroup
#  x2 <- TwitMatList[[m]] %>% group_by(SYMBOL) %>% 
#    summarise(ClusterID = first(ClusterID)) %>% left_join(., x, by = "ClusterID") %>%
#    mutate(PeriodID = m)

```


Convert data to a data frame with the SYMBOL the Period number the ClusterID, and the SI Score
```{r}
Bullscore <- TwitMatList(TwitRetweets, Metric =  names(TwitRetweets)[4])
x <- Aggregator(Bullscore, Metric = names(TwitRetweets)[4])

ggplot(x, aes(x= PeriodID, y = mean, colour = SYMBOL)) + geom_line() +
  ggtitle("Comparing Unweighted Social Index behaviour across time") +xlab("Period Number")+ 
  theme(legend.position = "none") + ylab(paste("Mean", names(TwitRetweets)[5]))
```

Create a list of all time periods for all metrics to compare the number of clusters
```{r}

MetricList <- lapply(c(4:5,7:9), function(z){
  Score <- TwitMatList(TwitRetweets, Metric = names(TwitRetweets)[z])
x <- Aggregator(Score, Metric = names(TwitRetweets)[z])
}
                     )
names(MetricList) <-names(TwitRetweets)[c(4:5,7:9)]
setwd(DataFiles)
saveRDS(MetricList, "MetricList.rds")


setwd(GraphicsFiles)

n <-2
ggplot(MetricList[[n]], aes(x= PeriodID, y = mean, colour = SYMBOL)) + geom_line() +
  ggtitle("Comparing Unweighted Social Index behaviour across time") +xlab("Period Number")+ 
  theme(legend.position = "none") + ylab(paste("Mean", names(MetricList[n])))

```

Extract number of clusters period
```{r}

x <- sapply(1:5, function(n){
  x <- MetricList[[n]] %>% group_by(PeriodID) %>% 
    summarise(TotalClusters = length(unique(ClusterID))) %>% ungroup %>%
    select(TotalClusters) 
    names(x) <- names(MetricList[n])
    x
    }
)

ClusterPeriod <- do.call(cbind, x) %>% as.data.frame %>% mutate(PeriodID = 1:nrow(.)) %>%
  gather(key = Metric, value=Clusters, -PeriodID)

setwd(GraphicsFiles)
ggplot(ClusterPeriod, aes(x= PeriodID, y= Clusters, colour = Metric)) + geom_line() +
  ggtitle("Total Number of Clusters per Metric") + 
  theme(legend.title= "left", legend.position = "bottom")
ggsave("ClusterNumbers.png", width = 911/3, height = 512/3, units ="mm" )
```


Plot unweighted Social Index across periods
```{r}
metric <- names(TwitRetweets)[5]

Bullscore <- TwitMatList(TwitRetweets, Metric = metric )
x <- Aggregator(Bullscore, Metric = metric)


target <-  x$SYMBOL %in% c("AXP", "AAPL", "CSCO", "IBM", "INTC", "JNJ", "JPM", "MSFT", "NKE", "V", "DIS")
 
ggplot(x[target,], aes(x= PeriodID, y = mean, colour = SYMBOL)) + geom_line() +
  ggtitle("Comparing Unweighted Social Index behaviour across time") + ylab(paste("Mean",names(TwitRetweets)[5])) +xlab("Period Number")

ggplot(x, aes(x= PeriodID, y = mean, colour = SYMBOL)) + geom_line() +
  ggtitle("Comparing Unweighted Social Index behaviour across time") + ylab(paste("Mean",names(TwitRetweets)[5])) +xlab("Period Number") +theme(legend.position = "none")
ggsave("AllperiodsAllSymbols.png")
```

Identify time series in all periods by metric
```{r}

.dots = setNames(dots, c("mean", "count"))
dots <- list( ~length(unique(PeriodID)))

x <- lapply(1:5, function(z) {
  MetricList[[z]] %>% group_by(SYMBOL) %>% 
    summarise_( . , .dots = setNames(dots, names(MetricList[z]) )) %>% ungroup
    }
)

x2 <- full_join( x[[1]], x[[2]], by = "SYMBOL") %>% 
  full_join( ., x[[3]], by = "SYMBOL") %>%
  full_join( ., x[[4]], by = "SYMBOL") %>%
  full_join( ., x[[5]], by = "SYMBOL")


```


Import symbol time series from Quandl
```{r}
setwd(DataFiles)
AppleReturns <- read.csv("AppleReturns.csv") %>% 
  left_join(.,read.csv("Marketindex.csv") ) %>% 
  bind_cols(.,MetricList[[2]] %>% filter(SYMBOL == "AAPL") ) %>% 
  rename(Return = Adj..Close, MI = Adj.Close, CSI = mean) %>%
  select(-ClusterID, -PeriodID, -SYMBOL) %>% mutate_each(funs(scale), -Date) 

AppleReturnsplt <- AppleReturns %>%  gather(key = Variable, value = Norm_Ret, -Date)

setwd(GraphicsFiles)
ggplot(AppleReturnsplt, aes(x = Date, y= Norm_Ret, group =(Variable), colour = Variable)) +geom_point()+geom_line() +ggtitle("Comparison of the CSI \n the MI and Returns for Apple") +
  ylab("Normalised Values") +theme(axis.text.x = element_text(angle = 45))
ggsave("ApplePlot.png")

x <- AppleReturns %>% select(-Date) %>% as.matrix %>% rcorr

```



Structure graph for use in gephi with as much metadata as possible
symbol, cluster ID, edge type (aka inter cluster vs intra cluster), is target symbol/node yesy/no etc
```{r}
n <- 27
data = TwitBullvar[periodID == n,]
  remove = nearZeroVar(data)
  if(length(remove) >0){
  data = data[,-remove]
  }
  corlist = data[,2:ncol(data)] %>% as.matrix %>% rcorr
  distmat = sqrt(2*(1-corlist[[1]]))
  sigmat = corlist[[3]]>0.90
  MNet = distmat*sigmat #weighted undirected adjacency matrix
  #MNet = graph.adjacency(MNet, mode = "undirected", weighted = TRUE, diag = FALSE)#do not use if clustering with mcl
  MNet2 <- MNet
  com5 = mcl(x = MNet2, addLoops=TRUE, ESM = FALSE)
  symclust = data.frame(SYMBOL = names(data)[-1], ClusterID = com5$Cluster)
  Twitaggmat = TwitRetweets %>% select(SYMBOL, TIMESTAMP_UTC, Metric) %>% 
    inner_join(., symclust, "SYMBOL")

  
MNet = graph.adjacency(MNet, mode = "undirected", weighted = TRUE, diag = FALSE)#do not use if   
MNet <- set.vertex.attribute(MNet, name = "Cluster", index = V(MNet), value = com5$Cluster)
list.vertex.attributes(MNet)

noclust <-4
#colour nodes by cluster
test <- table(com5$Cluster) %>% data.frame %>% rename(ClusterID = Var1, Nodes = Freq) %>%
  mutate(rank = rank(-Nodes, ties.method = "min"), 
         rank = ifelse(ClusterID==0,1000,rank-1),
         colour = "None", 
         colour = ifelse(Nodes >noclust,"small", colour),
         colour = ifelse(ClusterID==0,"None",colour),
         colour =  ifelse(rank ==1 & Nodes >noclust, "first", colour), 
         colour = ifelse(rank == 2 & Nodes >noclust, "second", colour), 
         colour = ifelse(rank == 3 & Nodes >noclust, "third", colour))


V(MNet)$fill<-test$colour[match(com5$Cluster,test$ClusterID)]

Perclusts <- TwitMatAgg %>% filter(PeriodID == n) %>% select(SYMBOL, ClusterID) 
Perclusts2 <- Perclusts %>% rename(SYMBOL2 = SYMBOL)
edgecolours <-get.edgelist(MNet) %>% data.frame %>% rename(SYMBOL = X1, SYMBOL2 = X2) %>%
  left_join(., Perclusts, by = "SYMBOL") %>%left_join(., Perclusts2, by = "SYMBOL2") %>%
  mutate(colour = ifelse(ClusterID.x == ClusterID.y, "00,255,00", "194,194,194"))

E(MNet)$colour <- edgecolours$colour
#save graph for import to gephi
setwd(DataFiles)
write.graph(MNet, "test.graphml", format = "graphml")


```

