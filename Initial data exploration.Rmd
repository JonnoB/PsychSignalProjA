---
title: "Data analytics summary"
author: "Jonathan Bourne"
date: "1 February 2016"
output: html_document
---



```{r}
packages <- c("dplyr", "tidyr", "ggplot2", "caret")
sapply(packages, library, character.only = TRUE)

setwd("C:/Users/Jonno/Dropbox/Data_Analytics/Data") #change this to your file path

#TwitRetweets <- read.csv(gzfile("twitter_withretweets_daily.csv.gz"), as.is = TRUE)
TwitNoRetweets <- read.csv("twitter_noretweets_daily.csv", as.is = TRUE)

#stocktwits <- read.csv(gzfile("stocktwits_daily.csv.gz"), as.is = TRUE)


```


total number of time units and companies
```{r}
length(unique(TwitNoRetweets$SYMBOL)) %>% paste("Number of unique symbols")
length(unique(TwitNoRetweets$TIMESTAMP_UTC)) %>% paste("Number of unique timestamps", .)
```

summary of each variable

```{r}
summary(TwitNoRetweets)
```


Density plots of all variables except Source, Symbol, Timestamp utc,

The density plots show that almost all the variables are highly skewes except for bull minus bear which is roughly symmetrical. 
```{r}

sapply(c(1:10)[-c(1:3)], function(n) {
  
  plot(density(TwitNoRetweets[,n]), main = names(TwitNoRetweets)[n])
})


```


log Density plots +1 of all variables except Source, Symbol, Timestamp utc, Bull minus Bear

Because of the high level of skew log density plots could give us more information, 1 was added to all values to prevent log zero errors, as there are a lot of zeros

Bull and bear scored messages as well as total scanned messages follow an attenuating peaked pattern
```{r}

sapply(c(1:10)[-c(1:3,6)], function(n) {
  
  plot(density(log10(TwitNoRetweets[,n]+1)), main = names(TwitNoRetweets)[n])
})


```

Removes TwitNoRetweets to save ram
```{r}
rm(TwitNoRetweets)
```


Aggregate to a reasonable number of groups
plot vs time
Plot densities

bull bear vs index returns.

Cumulative stock tweets per cash tag. does the top 10percent rep 90 % of the whole thing

change in tweet volume over time. 
is there a difference in change in tweet volume by industry.

Load symbology
```{r}
symbology <- read.csv("symbology.csv")
```

```{r}
summary(symbology)
```


```{r}
length(unique(symbology$symbol)) %>% paste("Number of unique symbols")
length(unique(symbology$exchange)) %>% paste("Number of unique exchanges", .)
length(unique(symbology$sector)) %>% paste("Number of unique sectors", .)
length(unique(symbology$industry)) %>% paste("Number of unique industries", .)

```


Remove Low variance variables, and high correllation variables
```{r}
corprep <- symbology[,6:47]
constantVars <- corprep[,nearZeroVar(corprep)] %>% names
corprep <- corprep[,-nearZeroVar(corprep)]
#names(corprep) <- paste("X",1:ncol(corprep), sep="")
x <- cor(corprep)
highCorNames <- corprep[,c(findCorrelation(x, 0.9))] %>% names


x <- table(SymbologyLowCor$exchange) %>% data.frame %>% rename(exchange = Var1) %>% 
  left_join(. , ExchGath2, by = "exchange") %>% 
  mutate_each(funs(perc = ./sum(.)), -exchange)

```


Twitter scanned tweets
```{r}
twitscan <- symbology %>% select(exchange, 
                                 contains("twitter_scanned"), 
                                 -contains("January"), 
                                 -contains("YTD")) %>% 
  gather(key = year, value = scanned, -exchange ) %>% 
  mutate( year= gsub("twitter_scanned_","",twitscan$year )) #I can't remove text around the year in one stage for some reason.

twitscan <- twitscan %>%  mutate(year= gsub("_avg_monthly","",twitscan$year ) %>%as.integer ) 

ggplot(twitscan, aes(x= exchange, y  =log10(scanned+1) )) +geom_boxplot() +ggtitle("Average monthly scanned tweets across \nall symbols and years broken out by Exchange") + 
  theme(axis.text.x = element_text(angle = 45))


twitscanagg <- twitscan %>% group_by(exchange, year) %>% summarise(scanned = sum(scanned))

ggplot(twitscanagg, aes(x = year, y= log10(scanned), colour = exchange)) +geom_line() +ggtitle("Number of scanned tweets \nacross all years broken out by exchange")

```

Twitter scored tweets
```{r}
twitscore <- symbology %>% select(exchange, 
                                 contains("twitter_scored"), 
                                 -contains("January"), 
                                 -contains("YTD")) %>% 
  gather(key = year, value = scored, -exchange ) 
twitscore  %<>% mutate( year= gsub("twitter_scored_","",twitscore$year )) #I can't remove text around the year in one stage for some reason.
twitscore %<>%  mutate(year= gsub("_avg_monthly","",twitscore$year ) %>%as.integer ) 

ggplot(twitscore, aes(x= exchange, y  =log10(scored+1) )) +geom_boxplot() +ggtitle("Average monthly scored tweets across\n all symbols and years broken out by Exchange") + 
  theme(axis.text.x = element_text(angle = 45))


twitscoreagg <- twitscore %>% group_by(exchange, year) %>% summarise(scored = sum(scored))

ggplot(twitscoreagg, aes(x = year, y= log10(scored), colour = exchange)) +geom_line() +ggtitle("Number of scored tweets \nacross all years broken out by exchange")



```


Plot the percentage of dataset volume by stock twits, twitter month data per year
y = percent, x = year, facet = tweet/stocktwit, colour = exchange

We want to group by exchange then summarise total counts and average tweets, then mutate and create a percentages variable then plot the ordered cumulative with horizontal line at 80 and a verticle line dropping at the intersect

Also an overlapping plot wich shows the number of companies and the average number of tweets per exchange and sector.

Provide reason to remove various companies excahnges to reduce the total number of companies to track. mostly low tweet activity e.g average tweets less than 30, or just look at the top symbols

  