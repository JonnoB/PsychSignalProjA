---
title: "Data analytics summary"
author: "Jonathan Bourne"
date: "1 February 2016"
output: html_document
---

The Goal of this document is to provide summary statistics for Milestone 1 of the Data Analytics project with PsychSignal


```{r}
packages <- c("dplyr", "tidyr", "ggplot2", "caret", "corrplot", "xtable", "magrittr")
sapply(packages, library, character.only = TRUE)


basewd <- "C:/Users/Jonno/Dropbox/Data_Analytics" #change this to your dropbox file path for data analytics
DataFiles <- file.path(basewd, "Data")
GraphicsFiles <- file.path(basewd, "Graphics", "Milestone1")

#TwitRetweets <- read.csv(gzfile("twitter_withretweets_daily.csv.gz"), as.is = TRUE)

#stocktwits <- read.csv(gzfile("stocktwits_daily.csv.gz"), as.is = TRUE)


```

Create Empty data frame with required variables
```{r}

DataSetsInfo <- data.frame(Name = character(), Observations = integer(), Symbols = integer(), TimePeriodStart = character(),TimePeriodEnd = character(), TimePeriods = integer(), TotalVariables = integer(), MeanScanned = integer(), MedianScanned = integer(), SDScanned = integer())
```

total number of time units and companies
```{r}
setwd(DataFiles)
Data <- read.csv("twitter_noretweets_daily.csv", as.is = TRUE)

tempdat <- data.frame(Name = "TwitNoRetweets",
                       Observations = nrow(Data), 
                       Symbols = length(unique(Data$SYMBOL)), 
                       TimePeriodStart = min(Data$TIMESTAMP_UTC),
                       TimePeriodEnd = max(Data$TIMESTAMP_UTC), 
                       TimePeriods = length(unique(Data$TIMESTAMP_UTC)),
                       TotalVariables = ncol(Data),
                       MeanScanned = mean(Data$TOTAL_SCANNED_MESSAGES) %>% round,
                       MedianScanned = median(Data$TOTAL_SCANNED_MESSAGES),
                       SDScanned = sd(Data$TOTAL_SCANNED_MESSAGES) %>% round
                      )

DataSetsInfo %<>% rbind(., tempdat)

Data<- read.csv(gzfile("twitter_withretweets_daily.csv.gz"), as.is = TRUE)

tempdat <- data.frame(Name = "TwitRetweets",
                       Observations = nrow(Data), 
                       Symbols = length(unique(Data$SYMBOL)), 
                       TimePeriodStart = min(Data$TIMESTAMP_UTC),
                       TimePeriodEnd = max(Data$TIMESTAMP_UTC), 
                       TimePeriods = length(unique(Data$TIMESTAMP_UTC)),
                       TotalVariables = ncol(Data),
                       MeanScanned = mean(Data$TOTAL_SCANNED_MESSAGES) %>% round,
                       MedianScanned = median(Data$TOTAL_SCANNED_MESSAGES),
                       SDScanned = sd(Data$TOTAL_SCANNED_MESSAGES) %>% round
                      )
           
DataSetsInfo %<>% rbind(., tempdat)

Data<- read.csv(gzfile("stocktwits_daily.csv.gz"), as.is = TRUE)

tempdat <- data.frame(Name = "stocktwits",
                       Observations = nrow(Data), 
                       Symbols = length(unique(Data$SYMBOL)), 
                       TimePeriodStart = min(Data$TIMESTAMP_UTC),
                       TimePeriodEnd = max(Data$TIMESTAMP_UTC), 
                       TimePeriods = length(unique(Data$TIMESTAMP_UTC)),
                       TotalVariables = ncol(Data),
                       MeanScanned = mean(Data$TOTAL_SCANNED_MESSAGES) %>% round,
                       MedianScanned = median(Data$TOTAL_SCANNED_MESSAGES),
                       SDScanned = sd(Data$TOTAL_SCANNED_MESSAGES) %>% round
                      )
           
DataSetsInfo %<>% rbind(., tempdat)

DataSetsInfo2 <- DataSetsInfo %>% mutate( TimePeriodStart = sub("T.*", "", .$TimePeriodStart), TimePeriodEnd = sub("T.*", "", .$TimePeriodEnd))

xtable(DataSetsInfo, label = "tab:DataSetsInfo", caption = "Key Elements from the time series datasets are shown in this table. The data are quite similar with a few notable differences. Stocktwits contains significantlt less observations than the other two datasets, but contains more time periods. The distribution of the tweets per time period is highly skewed, with the mean almost 10 time the median, and a very large Standard Deviation across all variables.")

DataSetsInfo2 <- DataSetsInfo2 %>% t  %>% data.frame
names(DataSetsInfo2) <- sapply(DataSetsInfo2[1,], as.character)
DataSetsInfo2 <- DataSetsInfo2[-1,]

xtable(DataSetsInfo2, label = "tab:DataSetsInfo", caption = "Key Elements from the time series datasets are shown in this table. The data are quite similar with a few notable differences. Stocktwits contains significantlt less observations than the other two datasets, but contains more time periods. The distribution of the tweets per time period is highly skewed, with the mean almost 10 time the median, and a very large Standard Deviation across all variables.")

```

Remove the datasets to save RAM
```{r}
rm(Data)
```


Load No retweets
```{r}
setwd(DataFiles)
TwitNoRetweets <- read.csv("twitter_noretweets_daily.csv", as.is = TRUE)
symbology <- read.csv("symbology.csv")
setwd(GraphicsFiles)
```


Merge exchanges into time series data
```{r}

Twit2 <- symbology %>% select(symbol,exchange) %>% 
  inner_join(.,TwitNoRetweets, by = c("symbol" ="SYMBOL" ))

Twit2 %<>% group_by(exchange, TIMESTAMP_UTC) %>% summarise(BULL_SCORED_MESSAGES = sum(BULL_SCORED_MESSAGES))

ggplot(Twit2 %>% filter(exchange == "NYSE") %>% head(21), 
       aes(x=TIMESTAMP_UTC, y=BULL_SCORED_MESSAGES %>% log10, colour = exchange)) +
  geom_line(aes(group=exchange)) +ggtitle("Visualising the Periodicity of the NYSE")


NYSE <-Twit2 %>% filter(exchange == "NYSE")

png("NYSE.png")
acf(NYSE$BULL_SCORED_MESSAGES)
dev.off()

x <- decompose(NYSE$BULL_SCORED_MESSAGES)
```


summary of each variable

```{r}
summary(TwitNoRetweets)
```


Density plots of all variables except Source, Symbol, Timestamp utc,

The density plots show that almost all the variables are highly skewes except for bull minus bear which is roughly symmetrical. 
```{r}

sapply(c(1:10)[-c(1:3)], function(n) {
  
  plot(density(TwitNoRetweets[,n]), main = names(TwitNoRetweets)[n])
})


```


log Density plots +1 of all variables except Source, Symbol, Timestamp utc, Bull minus Bear

Because of the high level of skew log density plots could give us more information, 1 was added to all values to prevent log zero errors, as there are a lot of zeros

Bull and bear scored messages as well as total scanned messages follow an attenuating peaked pattern
```{r}

sapply(c(1:10)[-c(1:3,6)], function(n) {
  
  plot(density(log10(TwitNoRetweets[,n]+1)), main = names(TwitNoRetweets)[n])
})


```

Removes TwitNoRetweets to save ram
```{r}
rm(TwitNoRetweets)
```


bull bear vs index returns.


Load symbology
```{r}
setwd(DataFiles) 
symbology <- read.csv("symbology.csv")
setwd(GraphicsFiles) 
```

```{r}
summary(symbology)
```

Summary Table for symbols data
```{r}

symbolsDat <- data.frame(Symbols= nrow(symbology),
                       Exchanges = length(unique(symbology$exchange)),
                       Sectors = length(unique(symbology$sector)),
                       Industries = length(unique(symbology$industry)),
                       TotalVariables = ncol(symbology)
                       )

xtable(symbolsDat, caption = "Summary of the Symbols Dataset. This data set also includes aggregated data on Cashtag frequency per symbol. Tweet frequency is discussed elsewhere in this report.", label = "tab:symbolsDat")
```


Remove Low variance variables, and high correllation variables

High correlation variables can be included in a table as a point of note, also Low varience variables.
A corelation plot grouped using hierarchical clustering is also produced
```{r}
corprep <- symbology[,6:47]
constantVars <- corprep[,nearZeroVar(corprep)] %>% names
corprep <- corprep[,-nearZeroVar(corprep)]
#names(corprep) <- paste("X",1:ncol(corprep), sep="")
corsymb <- cor(corprep)
highCorNames <- corprep[,c(findCorrelation(corsymb, 0.9))] %>% names

corrplot(corsymb, method = "shade", order = "hclust", title = "Corellation plot of symbology variables", tl.pos = "n")
```


Twitter scanned tweets
```{r}
twitscan <- symbology %>% select(exchange, 
                                 contains("twitter_scanned"), 
                                 -contains("January"), 
                                 -contains("YTD")) %>% 
  gather(key = year, value = scanned, -exchange ) %>% 
  mutate( year= gsub("twitter_scanned_","",.$year ) %>% 
            gsub("_avg_monthly","", . ) %>%as.integer ) 

ggplot(twitscan, aes(x= exchange, y  =log10(scanned+1), fill = exchange )) +geom_boxplot() +ggtitle("Average monthly scanned tweets across \nall symbols and years broken out by Exchange") + theme(axis.text.x = element_text(angle = 45))+ guides(fill=FALSE)
ggsave("ScannedTweetsBoxplotByExchange.png")

twitscanagg <- twitscan %>% group_by(exchange, year) %>% summarise(scanned = sum(scanned))

ggplot(twitscanagg, aes(x = year, y= log10(scanned), colour = exchange)) +geom_line() +ggtitle("Number of scanned tweets \nacross all years broken out by exchange")
ggsave("ScannedTweetsLineplotByExchange.png")


```

Twitter scored tweets
```{r}
twitscore <- symbology %>% select(exchange, 
                                 contains("twitter_scored"), 
                                 -contains("January"), 
                                 -contains("YTD")) %>% 
  gather(key = year, value = scored, -exchange ) %>%
  mutate( year= gsub("twitter_scored_","",.$year ) %>% 
            gsub("_avg_monthly","", . ) %>%as.integer)

ggplot(twitscore, aes(x= exchange, y  =log10(scored+1), fill = exchange )) +geom_boxplot() +ggtitle("Average monthly scored tweets across\n all symbols and years broken out by Exchange") + 
  theme(axis.text.x = element_text(angle = 45))+ guides(fill=FALSE)
ggsave("ScoredTweetsBoxplotByExchange.png")


twitscoreagg <- twitscore %>% group_by(exchange, year) %>% summarise(scored = sum(scored))

ggplot(twitscoreagg, aes(x = year, y= log10(scored), colour = exchange)) +geom_line() +ggtitle("Number of scored tweets \nacross all years broken out by exchange")
ggsave("ScoredTweetsLineplotByExchange.png")

```


Plot the cumulative amount of tweets against the cumulative rank, to find how many stocks we need to have 80% of the tweet volume, the data is taken from 2015
```{r}
twitscanvol <- symbology %>% select(symbol, all_scanned_2015_avg_monthly, exchange) %>% 
  rename(year_2015= all_scanned_2015_avg_monthly) %>%
  arrange(-year_2015) %>%
  mutate(cumtwits = cumsum(year_2015), CumPercTwit = cumtwits/sum(year_2015) ,CumPercRank = rownames(.) %>% as.integer %>% percent_rank)

ggplot(twitscanvol, aes(x= CumPercRank, y = CumPercTwit, colour= exchange)) +geom_point()+ggtitle("Cumulative percent of tweets vs \n Cumulative rank of symbol")
ggsave("TweetVolCumLineplot.png")


criticalsymb1 <-twitscanvol %>% droplevels %>% group_by(exchange) %>%
  summarise(tweets = sum(year_2015), NumSymbols = n()) %>%
  arrange(-tweets) %>% 
  mutate(PercTweets = tweets/sum(tweets) , PercTweets=round(PercTweets,digits= 2),
         CumPercTwit = cumsum(PercTweets),
         CumPercSymb = NumSymbols/sum(NumSymbols),
         CumPercSymb = cumsum(CumPercSymb) %>% round(., digits = 2))


xtable(criticalsymb, caption="Sum of Average monthly Twitter volume per symbol agreggated by exchange, NYSE, NASDAQ and NYSEArca account for over 70\\% of all symbols, and 90\\% of all Twitter volume", label = "tab:twittervolume") #for output to the Latex doc

x <- findInterval(0.8, twitscanvol$CumPercTwit) #index to take from

criticalsymb <- twitscanvol[1:x,] %>% droplevels %>% group_by(exchange) %>%
  summarise(tweets = sum(year_2015), NumSymbols = n()) %>%
  arrange(-tweets) %>% 
  mutate(PercTweets = tweets/sum(tweets) , PercTweets=round(PercTweets,digits= 2),
         CumPercTwit = cumsum(PercTweets),
         CumPercSymb = NumSymbols/sum(NumSymbols),
         CumPercSymb = cumsum(CumPercSymb) %>% round(., digits = 2))

xtable(criticalsymb, caption="Breakdown of 80\\% of the scanned tweet volume in 2015 aggregated by exchange, The NYSE, NASDAQ and NYSEArca are still the dominant excahnges for twitter volume",
       label = "tab:twittervolume80perc") #for output to the Latex doc


# ggplot(criticalsymb1, aes(x= CumPercSymb, y = CumPercTwit)) +geom_line()+ggtitle("Cumulative percent of tweets vs \n Cumulative rank of exchange for top 80% of data")

```


Plot the percentage of dataset volume by stock twits, twitter month data per year
y = percent, x = year, facet = tweet/stocktwit, colour = exchange

We want to group by exchange then summarise total counts and average tweets, then mutate and create a percentages variable then plot the ordered cumulative with horizontal line at 80 and a verticle line dropping at the intersect

Also an overlapping plot wich shows the number of companies and the average number of tweets per exchange and sector.

Provide reason to remove various companies excahnges to reduce the total number of companies to track. mostly low tweet activity e.g average tweets less than 30, or just look at the top symbols

  