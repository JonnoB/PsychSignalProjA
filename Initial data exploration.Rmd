---
title: "Data analytics summary"
author: "Jonathan Bourne"
date: "1 February 2016"
output: html_document
---



```{r}
packages <- c("dplyr", "tidyr", "ggplot2", "caret")
sapply(packages, library, character.only = TRUE)

setwd("C:/Users/Jonno/Dropbox/Data_Analytics/Data") #change this to your file path

#TwitRetweets <- read.csv(gzfile("twitter_withretweets_daily.csv.gz"), as.is = TRUE)
TwitNoRetweets <- read.csv("twitter_noretweets_daily.csv", as.is = TRUE)

#stocktwits <- read.csv(gzfile("stocktwits_daily.csv.gz"), as.is = TRUE)


```


total number of time units and companies
```{r}
length(unique(TwitNoRetweets$SYMBOL)) %>% paste("Number of unique symbols")
length(unique(TwitNoRetweets$TIMESTAMP_UTC)) %>% paste("Number of unique timestamps", .)
```

summary of each variable

```{r}
summary(TwitNoRetweets)
```


Density plots of all variables except Source, Symbol, Timestamp utc,

The density plots show that almost all the variables are highly skewes except for bull minus bear which is roughly symmetrical. 
```{r}

sapply(c(1:10)[-c(1:3)], function(n) {
  
  plot(density(TwitNoRetweets[,n]), main = names(TwitNoRetweets)[n])
})


```


log Density plots +1 of all variables except Source, Symbol, Timestamp utc, Bull minus Bear

Because of the high level of skew log density plots could give us more information, 1 was added to all values to prevent log zero errors, as there are a lot of zeros

Bull and bear scored messages as well as total scanned messages follow an attenuating peaked pattern
```{r}

sapply(c(1:10)[-c(1:3,6)], function(n) {
  
  plot(density(log10(TwitNoRetweets[,n]+1)), main = names(TwitNoRetweets)[n])
})


```

Removes TwitNoRetweets to save ram
```{r}
rm(TwitNoRetweets)
```


Aggregate to a reasonable number of groups
plot vs time
Plot densities

bull bear vs index returns.

Cumulative stock tweets per cash tag. does the top 10percent rep 90 % of the whole thing

change in tweet volume over time. 
is there a difference in change in tweet volume by industry.

Load symbology
```{r}
symbology <- read.csv("symbology.csv")
```

```{r}
summary(symbology)
```


```{r}
length(unique(symbology$symbol)) %>% paste("Number of unique symbols")
length(unique(symbology$exchange)) %>% paste("Number of unique exchanges", .)
length(unique(symbology$sector)) %>% paste("Number of unique sectors", .)
length(unique(symbology$industry)) %>% paste("Number of unique industries", .)

```


Remove Low variance variables, and high correllation variables
```{r}
corprep <- symbology[,6:47]
corprep <- corprep[,-nearZeroVar(corprep)]
names(corprep) <- paste("X",1:ncol(corprep), sep="")
x <- cor(corprep)
SymbologyLowCor <- symbology[,-c(findCorrelation(x, 0.9)+5)] #cut off 0.9

```

Absolute values of exchange, sector symbol
```{r}
exchange <- table(SymbologyLowCor$exchange) %>% data.frame %>% arrange(-Freq)
ggplot(exchange, aes(x=Var1)) +geom_bar(stat = "count")

ggplot(SymbologyLowCor, aes(x=exchange)) +geom_bar()
```

We want to group by exchange then summarise total counts and average tweets, then mutate and create a percentages variable then plot the ordered cumulative with horizontal line at 80 and a verticle line dropping at the intersect

Also an overlapping plot wich shows the number of companies and the average number of tweets per exchange and sector.

Provide reason to remove various companies excahnges to reduce the total number of companies to track. mostly low tweet activity e.g average tweets less than 30, or just look at the top symbols
